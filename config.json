{
  "version": "1.0.0",
  "name": "ToolBridge",
  "description": "Universal LLM Tool Proxy Server with advanced stream processing and format conversion",
  "_comments": {
    "configuration": "This file contains the configuration (modes, ports, timeouts). Sensitive values (API keys, credentials) go in .env file.",
    "servingMode": "What API format ToolBridge serves to clients: 'openai' or 'ollama'",
    "backendMode": "What type of LLM provider your backend is: 'openai' or 'ollama'",
    "passTools": "When true, original tools/tool_choice fields are kept in backend payload. When false, they are removed. Tool instructions are ALWAYS added to system messages regardless."
  },
  "server": {
    "servingMode": "ollama",
    "defaultHost": "0.0.0.0",
    "defaultPort": 3000,
    "defaultDebugMode": true
  },
  "backends": {
    "defaultMode": "ollama",
    "supportedModes": [
      "openai",
      "ollama"
    ],
    "defaultChatPath": "/chat/completions",
    "defaultBaseUrls": {
      "openai": "https://openrouter.ai/api/v1",
      "ollama": "http://localhost:11434"
    },
    "ollama": {
      "defaultContextLength": 32768,
      "defaultUrl": "http://localhost:11434"
    }
  },
  "tools": {
    "enableReinjection": true,
    "reinjectionMessageCount": 3,
    "reinjectionTokenCount": 1000,
    "reinjectionType": "system",
    "maxIterations": 5,
    "passTools": false
  },
  "performance": {
    "maxBufferSize": 1048576,
    "connectionTimeout": 120000,
    "maxStreamBufferSize": 1048576,
    "streamConnectionTimeout": 120000
  },
  "headers": {
    "httpReferer": "https://github.com/Oct4Pie/toolbridge",
    "xTitle": "toolbridge"
  },
  "validation": {
    "requiredEnvVars": {
      "openai": [
        "BACKEND_LLM_API_KEY"
      ],
      "ollama": []
    },
    "placeholders": {
      "apiKey": "YOUR_API_KEY_HERE"
    }
  },
  "testing": {
    "models": {
      "openai": "gpt-4o",
      "ollama": "qwen3:latest"
    }
  }
}