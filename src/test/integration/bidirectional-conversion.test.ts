/**
 * BIDIRECTIONAL CONVERSION TESTS
 * 
 * Tests ALL format conversions in BOTH directions:
 * - OpenAI âŸ· Ollama
 * 
 * Each backend can serve requests from any client format
 * NO MOCKS - 100% real API calls
 */

import { spawn } from "child_process";

import { expect } from "chai";
import { after, before, describe, it } from "mocha";
import OpenAI from "openai";

import type { ChildProcess } from "child_process";

// Environment configuration
const PROXY_PORT = parseInt(process.env.PROXY_PORT ?? "3000", 10);
const PROXY_HOST = process.env.PROXY_HOST ?? "localhost";

const TEST_MODEL_OPENROUTER = process.env["TEST_MODEL_OPENROUTER"] ?? "deepseek/deepseek-chat-v3.1:free";
const TEST_MODEL_OLLAMA = process.env["TEST_MODEL_OLLAMA"] ?? "llama3.2:1b";
const BACKEND_API_KEY = process.env.BACKEND_LLM_API_KEY ?? "sk-test";

console.log("\nðŸ”„ BIDIRECTIONAL CONVERSION TESTS");
console.log("===================================");
console.log(`Proxy: http://${PROXY_HOST}:${PROXY_PORT}`);
console.log(`OpenRouter: ${TEST_MODEL_OPENROUTER}`);
console.log(`Ollama: ${TEST_MODEL_OLLAMA}`);
console.log("");

describe("ðŸ”„ BIDIRECTIONAL: All Format Conversions", function() {
  this.timeout(120000);

  let proxyServer: ChildProcess | null = null;
  let ollamaAvailable = false;

  before(async function() {
    this.timeout(15000);
    
    console.log("Starting ToolBridge proxy server...");
    
    proxyServer = spawn("node", ["dist/src/index.js"], {
      env: {
        ...process.env,
        PROXY_PORT: PROXY_PORT.toString(),
        DEBUG_MODE: "false",
        PASS_TOOLS: "false",
      },
      stdio: ["ignore", "pipe", "pipe"]
    });

    await new Promise((resolve) => setTimeout(resolve, 3000));
    
    // Check Ollama availability
    try {
      const response = await fetch("http://localhost:11434/api/tags");
      if (response.ok) {
        ollamaAvailable = true;
        console.log("âœ“ Ollama available");
      }
    } catch {
      console.log("âœ— Ollama not available");
    }

    console.log("âœ“ ToolBridge proxy started\n");
  });

  after(() => {
    if (proxyServer) {
      proxyServer.kill();
      console.log("\nâœ“ ToolBridge proxy stopped");
    }
  });

  describe("1ï¸âƒ£ OpenAI SDK â†’ OpenRouter Backend", () => {
    it("should handle OpenAI format request", async () => {
      const client = new OpenAI({
        baseURL: `http://${PROXY_HOST}:${PROXY_PORT}/v1`,
        apiKey: BACKEND_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: TEST_MODEL_OPENROUTER,
        messages: [
          { role: "user", content: "Say 'OpenAI format works'" }
        ],
        max_tokens: 50,
      });

      console.log("OpenAIâ†’OpenRouter:", response.choices[0]?.message.content);

      expect(response.choices[0]?.message).to.exist;
      expect(response.choices[0]?.message.content).to.be.a("string");
    });

    it("should handle OpenAI streaming", async function() {
      this.timeout(30000);
      
      const client = new OpenAI({
        baseURL: `http://${PROXY_HOST}:${PROXY_PORT}/v1`,
        apiKey: BACKEND_API_KEY,
      });

      const stream = await client.chat.completions.create({
        model: TEST_MODEL_OPENROUTER,
        messages: [
          { role: "user", content: "Count: 1, 2, 3" }
        ],
        stream: true,
        max_tokens: 50,
      });

      const chunks: string[] = [];

      const streamPromise = (async () => {
        try {
          let count = 0;
          for await (const chunk of stream) {
            if (chunk.choices[0]?.delta?.content) {
              chunks.push(chunk.choices[0].delta.content);
            }
            if (++count > 200) break;
          }
        } catch (error) {
          console.log("Stream error:", (error as Error).message);
        }
      })();

      const timeoutPromise = new Promise<void>((resolve) => {
        setTimeout(resolve, 25000);
      });

      await Promise.race([streamPromise, timeoutPromise]);

      console.log("OpenAI streaming chunks:", chunks.length);
      
      if (chunks.length > 0) {
        expect(chunks.length).to.be.greaterThan(0);
      } else {
        this.skip();
      }
    });
  });

  describe("2ï¸âƒ£ Ollama Format â†’ OpenRouter Backend", () => {
    it("should convert Ollama /api/chat to OpenAI format", async () => {
      const response = await fetch(`http://${PROXY_HOST}:${PROXY_PORT}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: TEST_MODEL_OPENROUTER,
          messages: [
            { role: "user", content: "Say 'Ollama format works'" }
          ],
          stream: false,
        }),
      });

      expect(response.ok).to.be.true;
      const data = await response.json();

      console.log("Ollamaâ†’OpenRouter response format:", Object.keys(data));
      console.log("Content:", data.message?.content || data.response || data.choices?.[0]?.message?.content);

      expect(data).to.exist;
      
      // Response could be in either format depending on conversion
      const hasContent = data.message?.content || data.response || data.choices?.[0]?.message?.content;
      expect(hasContent).to.exist;
    });

    it("should handle Ollama streaming format", async () => {
      const response = await fetch(`http://${PROXY_HOST}:${PROXY_PORT}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: TEST_MODEL_OPENROUTER,
          messages: [
            { role: "user", content: "Count to 3" }
          ],
          stream: true,
        }),
      });

      expect(response.ok).to.be.true;
      
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      const chunks: string[] = [];

      if (reader) {
        try {
          let count = 0;
          while (count < 50) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const text = decoder.decode(value);
            chunks.push(text);
            count++;
          }
        } catch (error) {
          console.log("Stream error (may be expected):", (error as Error).message);
        }
      }

      console.log("Ollama streaming chunks received:", chunks.length);
      
      if (chunks.length > 0) {
        expect(chunks.length).to.be.greaterThan(0);
      }
    });
  });

  describe("3ï¸âƒ£ OpenAI SDK â†’ Ollama Backend (if available)", () => {
    it("should convert OpenAI format to Ollama native", async function() {
      if (!ollamaAvailable) {
        console.log("Skipping: Ollama not available");
        this.skip();
        return;
      }

      // Use OpenAI SDK but target Ollama model
      const client = new OpenAI({
        baseURL: `http://${PROXY_HOST}:${PROXY_PORT}/v1`,
        apiKey: "ollama",
      });

      const response = await client.chat.completions.create({
        model: TEST_MODEL_OLLAMA,
        messages: [
          { role: "user", content: "Say hello" }
        ],
        max_tokens: 50,
      });

      console.log("OpenAIâ†’Ollama:", response.choices[0]?.message.content);

      expect(response.choices[0]?.message).to.exist;
    });
  });

  describe("4ï¸âƒ£ Ollama Format â†’ Ollama Backend (if available)", () => {
    it("should handle native Ollama to Ollama", async function() {
      if (!ollamaAvailable) {
        console.log("Skipping: Ollama not available");
        this.skip();
        return;
      }

      const response = await fetch(`http://${PROXY_HOST}:${PROXY_PORT}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: TEST_MODEL_OLLAMA,
          messages: [
            { role: "user", content: "Say hello from Ollama" }
          ],
          stream: false,
        }),
      });

      expect(response.ok).to.be.true;
      const data = await response.json();

      console.log("Ollamaâ†’Ollama:", JSON.stringify(data, null, 2).substring(0, 200));

      expect(data).to.exist;
    });
  });
  describe("6ï¸âƒ£ Format Validation", () => {
    it("should preserve message structure in conversions", async () => {
      const client = new OpenAI({
        baseURL: `http://${PROXY_HOST}:${PROXY_PORT}/v1`,
        apiKey: BACKEND_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: TEST_MODEL_OPENROUTER,
        messages: [
          { role: "system", content: "You are a helpful assistant." },
          { role: "user", content: "Say hello" }
        ],
        max_tokens: 50,
      });

      console.log("Format validation response:", {
        id: response.id,
        model: response.model,
        hasChoices: response.choices.length > 0,
        hasMessage: !!response.choices[0]?.message,
        hasContent: !!response.choices[0]?.message.content,
      });

      expect(response).to.have.property("id");
      expect(response).to.have.property("model");
      expect(response).to.have.property("choices");
      expect(response.choices).to.have.lengthOf.at.least(1);
      expect(response.choices[0]).to.have.property("message");
    });

    it("should handle multimodal content arrays", async () => {
      const client = new OpenAI({
        baseURL: `http://${PROXY_HOST}:${PROXY_PORT}/v1`,
        apiKey: BACKEND_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: TEST_MODEL_OPENROUTER,
        messages: [
          {
            role: "user",
            content: [
              { type: "text", text: "Describe this" },
              { type: "text", text: "It's a simple test" }
            ]
          }
        ],
        max_tokens: 50,
      });

      console.log("Multimodal content response:", response.choices[0]?.message.content?.substring(0, 100));

      expect(response.choices[0]?.message).to.exist;
    });
  });

  describe("7ï¸âƒ£ Tool Calling Across Backends", () => {
    it("should inject XML tools for OpenRouter", async () => {
      const client = new OpenAI({
        baseURL: `http://${PROXY_HOST}:${PROXY_PORT}/v1`,
        apiKey: BACKEND_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: TEST_MODEL_OPENROUTER,
        messages: [
          { role: "user", content: "What's 10+5? Use calculator if available." }
        ],
        tools: [
          {
            type: "function",
            function: {
              name: "add",
              description: "Add two numbers",
              parameters: {
                type: "object",
                properties: {
                  a: { type: "number" },
                  b: { type: "number" }
                },
                required: ["a", "b"]
              }
            }
          }
        ],
        max_tokens: 200,
      });

      console.log("Tool calling response:", JSON.stringify(response.choices[0]?.message, null, 2));

      const message = response.choices[0]?.message;
      const content = message?.content ?? "";
      const hasXML = content.includes("<toolbridge:calls>") || content.includes("<add>");
      const hasToolCalls = message?.tool_calls && message.tool_calls.length > 0;

      console.log("Has XML:", hasXML);
      console.log("Has tool_calls:", hasToolCalls);
      console.log("Response:", content.substring(0, 200));

      expect(message).to.exist;
    });

    it("should inject XML tools via Ollama format", async () => {
      const response = await fetch(`http://${PROXY_HOST}:${PROXY_PORT}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: TEST_MODEL_OPENROUTER,
          messages: [
            { role: "user", content: "Multiply 6 times 7. Use multiply tool." }
          ],
          tools: [
            {
              type: "function",
              function: {
                name: "multiply",
                description: "Multiply numbers",
                parameters: {
                  type: "object",
                  properties: {
                    a: { type: "number" },
                    b: { type: "number" }
                  },
                  required: ["a", "b"]
                }
              }
            }
          ],
          stream: false,
        }),
      });

      expect(response.ok).to.be.true;
      const data = await response.json();

      console.log("Ollama format tool response:", JSON.stringify(data, null, 2).substring(0, 300));

      expect(data).to.exist;
      
      const content = data.message?.content || data.response || data.choices?.[0]?.message?.content || "";
      const hasXML = content.includes("<toolbridge:calls>") || content.includes("<multiply>");
      
      console.log("Ollama format has XML:", hasXML);
      console.log("Content:", content.substring(0, 200));
    });
  });

  describe("8ï¸âƒ£ Error Handling Across Formats", () => {
    it("should handle invalid model in OpenAI format", async () => {
      const client = new OpenAI({
        baseURL: `http://${PROXY_HOST}:${PROXY_PORT}/v1`,
        apiKey: BACKEND_API_KEY,
      });

      try {
        await client.chat.completions.create({
          model: "invalid-model-12345",
          messages: [
            { role: "user", content: "test" }
          ],
        });
        
        expect.fail("Should have thrown error");
      } catch (error) {
        console.log("OpenAI format error:", (error as Error).message.substring(0, 100));
        expect(error).to.exist;
      }
    });

    it("should handle invalid model in Ollama format", async () => {
      const response = await fetch(`http://${PROXY_HOST}:${PROXY_PORT}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "invalid-model-67890",
          messages: [
            { role: "user", content: "test" }
          ],
          stream: false,
        }),
      });

      console.log("Ollama format error status:", response.status);
      
      expect(response.ok).to.be.false;
      expect(response.status).to.be.oneOf([400, 404, 500]);
    });

    it("should handle malformed JSON in Ollama format", async () => {
      const response = await fetch(`http://${PROXY_HOST}:${PROXY_PORT}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: "{ invalid json",
      });

      console.log("Malformed JSON status:", response.status);
      
      expect(response.ok).to.be.false;
    });
  });
});
