/**
 * Comprehensive Format Conversion Test Suite
 * Tests type conversion across OpenAI and Ollama formats using real mock servers
 */

import axios from 'axios';
import { OpenAI } from 'openai';

import { startMockOllama } from './mock-ollama-server.js';
import { startMockOpenAI } from './mock-openai-server.js';

interface TestResult {
  name: string;
  success: boolean;
  error?: string;
  details?: Record<string, unknown>;
}

interface TestConfig {
  name: string;
  baseURL: string;
  headers?: Record<string, string>;
  format: 'openai' | 'ollama';
}

// Test configurations
const TEST_CONFIGS: TestConfig[] = [
  {
    name: 'OpenAI Format',
    baseURL: 'http://localhost:3001/v1',
    format: 'openai',
    headers: { 'Authorization': 'Bearer test-key' }
  },
  {
    name: 'Ollama Format',
    baseURL: 'http://localhost:11434',
    format: 'ollama'
  },
];

// Test messages and tools for comprehensive testing
const TEST_MESSAGES = [
  { role: 'system' as const, content: 'You are a helpful assistant with access to tools.' },
  { role: 'user' as const, content: 'What is the weather like in San Francisco?' },
];

const TEST_TOOLS = [{
  type: 'function' as const,
  function: {
    name: 'get_weather',
    description: 'Get current weather for a location',
    parameters: {
      type: 'object' as const,
      properties: {
        location: {
          type: 'string' as const,
          description: 'The city and state, e.g. San Francisco, CA',
        },
        unit: {
          type: 'string' as const,
          enum: ['celsius', 'fahrenheit'],
          description: 'Temperature unit',
        },
      },
      required: ['location' as const],
    },
  },
}];

// Test functions
async function testOpenAIClient(config: TestConfig): Promise<TestResult[]> {
  const results: TestResult[] = [];

  const pushFailure = (name: string, error: unknown): void => {
    results.push({
      name,
      success: false,
      error: error instanceof Error ? error.message : String(error),
    });
  };

  try {
    const client = new OpenAI({
      baseURL: config.baseURL,
      apiKey: 'test-key',
    });

    // Test 1: Basic chat completion
    try {
      console.log(`[${config.name}] Testing basic chat completion...`);
      const response = await client.chat.completions.create({
        model: 'gpt-4o',
        messages: TEST_MESSAGES,
        temperature: 0.7,
        max_tokens: 150,
      });

      results.push({
        name: `${config.name} - Basic Chat`,
        success: true,
        details: {
          id: response.id,
          model: response.model,
          finish_reason: response.choices?.[0]?.finish_reason,
          content: response.choices?.[0]?.message?.content,
          tokens: response.usage,
        },
      });
    } catch (error) {
      pushFailure(`${config.name} - Basic Chat`, error);
    }

    // Test 2: Chat completion with tools
    try {
      console.log(`[${config.name}] Testing chat completion with tools...`);
      const response = await client.chat.completions.create({
        model: 'gpt-4o',
        messages: TEST_MESSAGES,
        tools: TEST_TOOLS,
        tool_choice: 'auto',
        temperature: 0.5,
        max_tokens: 200,
      });

      const toolCalls = response.choices[0]?.message.tool_calls ?? [];

      results.push({
        name: `${config.name} - Tools Chat`,
        success: true,
        details: {
          id: response.id,
          model: response.model,
          has_tool_calls: toolCalls.length > 0,
          tool_calls: toolCalls,
          content: response.choices?.[0]?.message?.content,
          tokens: response.usage,
        },
      });
    } catch (error) {
      pushFailure(`${config.name} - Tools Chat`, error);
    }

    // Test 3: Streaming chat completion
    try {
      console.log(`[${config.name}] Testing streaming chat completion...`);
      const stream = await client.chat.completions.create({
        model: 'gpt-4o',
        messages: TEST_MESSAGES,
        stream: true,
        temperature: 0.8,
        max_tokens: 100,
      });

      let chunks = 0;
      let content = '';
      let finalUsage: unknown = null;

      for await (const chunk of stream) {
        chunks += 1;
        if (chunk.choices[0]?.delta?.content) {
          content += chunk.choices[0].delta.content;
        }
        if (chunk.usage) {
          finalUsage = chunk.usage;
        }
      }

      results.push({
        name: `${config.name} - Streaming Chat`,
        success: chunks > 0,
        details: {
          chunks_received: chunks,
          content_preview: content.slice(0, 100) + '...',
          final_usage: finalUsage,
        },
      });
    } catch (error) {
      pushFailure(`${config.name} - Streaming Chat`, error);
    }

    // Test 4: Streaming with tools
    try {
      console.log(`[${config.name}] Testing streaming with tools...`);
      const stream = await client.chat.completions.create({
        model: 'gpt-4o',
        messages: TEST_MESSAGES,
        tools: TEST_TOOLS,
        tool_choice: 'auto',
        stream: true,
        temperature: 0.6,
      });

      let chunks = 0;
      let toolCallChunks = 0;

      for await (const chunk of stream) {
        chunks += 1;
        if (chunk.choices[0]?.delta?.tool_calls) {
          toolCallChunks += 1;
        }
      }

      results.push({
        name: `${config.name} - Streaming Tools`,
        success: chunks > 0,
        details: {
          chunks_received: chunks,
          tool_call_chunks: toolCallChunks,
        },
      });
    } catch (error) {
      pushFailure(`${config.name} - Streaming Tools`, error);
    }

  } catch (error) {
    pushFailure(`${config.name} - Client Setup`, error);
  }

  return results;
}

async function testOllamaAPI(): Promise<TestResult[]> {
  const results: TestResult[] = [];
  const baseURL = 'http://localhost:11434';

  const pushFailure = (name: string, error: unknown): void => {
    results.push({
      name,
      success: false,
      error: error instanceof Error ? error.message : String(error),
    });
  };

  // Test 1: Native Ollama chat
  try {
    console.log('[Ollama Native] Testing native chat API...');
    const response = await axios.post(`${baseURL}/api/chat`, {
      model: 'llama3.1:8b',
      messages: TEST_MESSAGES,
      stream: false,
      temperature: 0.7,
      num_predict: 150,
    });

    results.push({
      name: 'Ollama Native - Chat',
      success: true,
      details: {
        model: response.data.model,
        content: response.data.message.content.slice(0, 100) + '...',
        eval_count: response.data.eval_count,
        prompt_eval_count: response.data.prompt_eval_count,
      },
    });
  } catch (error) {
    pushFailure('Ollama Native - Chat', error);
  }

  // Test 2: Native Ollama streaming (NDJSON)
  try {
    console.log('[Ollama Native] Testing native streaming...');
    const response = await axios.post(`${baseURL}/api/chat`, {
      model: 'llama3.1:8b',
      messages: TEST_MESSAGES,
      stream: true,
      temperature: 0.8,
      num_predict: 100,
    }, {
      responseType: 'stream',
    });

    let chunks = 0;
    let content = '';
    let finalStats: unknown = null;

    response.data.on('data', (chunk: Buffer) => {
      const lines = chunk.toString().split('\n').filter(line => line.trim());
      for (const line of lines) {
        try {
          const data = JSON.parse(line);
          chunks += 1;
          if (data.message?.content) {
            content += data.message.content;
          }
          if (data.done && data.eval_count) {
            finalStats = {
              eval_count: data.eval_count,
              prompt_eval_count: data.prompt_eval_count,
            };
          }
        } catch {
          // Ignore parse errors for partial chunks
        }
      }
    });

    await new Promise((resolve, reject) => {
      response.data.on('end', resolve);
      response.data.on('error', reject);
      setTimeout(() => reject(new Error('Timeout waiting for Ollama stream')), 10000);
    });

    results.push({
      name: 'Ollama Native - Streaming',
      success: chunks > 0,
      details: {
        chunks_received: chunks,
        content_preview: content.slice(0, 100) + '...',
        final_stats: finalStats,
      },
    });
  } catch (error) {
    pushFailure('Ollama Native - Streaming', error);
  }

  // Test 3: Ollama models list
  try {
    console.log('[Ollama Native] Testing models list...');
    const response = await axios.get(`${baseURL}/api/tags`);

    results.push({
      name: 'Ollama Native - Models',
      success: true,
      details: {
        models_count: response.data.models?.length ?? 0,
        models: response.data.models?.map((m: Record<string, unknown>) => m['name']) ?? [],
      },
    });
  } catch (error) {
    pushFailure('Ollama Native - Models', error);
  }

  return results;
}

async function runComprehensiveFormatTests(): Promise<void> {
  console.log('ðŸ§ª COMPREHENSIVE FORMAT CONVERSION TESTS');
  console.log('=' + '='.repeat(60));
  console.log('Starting mock servers and running comprehensive tests...');
  console.log('');

  // Start all mock servers
  const servers: Array<unknown> = [];

  try {
    console.log('ðŸ“¡ Starting mock servers...');
    servers.push(await startMockOpenAI(3001));
  servers.push(await startMockOllama(11434));
    console.log('âœ… All mock servers started successfully\\n');

    // Wait for servers to be ready
    await new Promise(resolve => setTimeout(resolve, 2000));

    // Run comprehensive tests
    const allResults: TestResult[] = [];

    // Test OpenAI format
    console.log('ðŸ” Testing OpenAI format...');
    const firstConfig = TEST_CONFIGS[0];
    if (firstConfig) {
      const openaiResults = await testOpenAIClient(firstConfig);
      allResults.push(...openaiResults);
    }

    // Test Ollama format  
    console.log('ðŸ” Testing Ollama format...');
    const ollamaResults = await testOllamaAPI();
    allResults.push(...ollamaResults);

    // Test through ToolBridge proxy (if running)
    console.log('ðŸ” Testing through ToolBridge proxy...');
    try {
      // Test OpenAI -> Ollama conversion
      const proxyClient = new OpenAI({
        baseURL: 'http://localhost:3000/v1',
        apiKey: 'test-key'
      });

      const proxyResponse = await proxyClient.chat.completions.create({
        model: 'gpt-4o',
        messages: TEST_MESSAGES,
        tools: TEST_TOOLS,
        temperature: 0.7
      });

      allResults.push({
        name: 'ToolBridge Proxy - Format Conversion',
        success: true,
        details: {
          proxy_working: true,
          response_id: proxyResponse.id,
          model: proxyResponse.model,
          has_response: !!proxyResponse.choices[0]?.message
        }
      });
    } catch (error) {
      allResults.push({
        name: 'ToolBridge Proxy - Format Conversion',
        success: false,
        error: `Proxy not running or error: ${error instanceof Error ? error.message : String(error)}`
      });
    }

    // Generate comprehensive report
    const separator = '='.repeat(61);
    console.log(`\n${separator}`);
    console.log('ðŸ“Š COMPREHENSIVE TEST RESULTS');
    console.log(separator);

    const successCount = allResults.filter(r => r.success).length;
    const totalCount = allResults.length;
    const successRate = Math.round((successCount / totalCount) * 100);

    console.log(`\nðŸ“ˆ Overall Success Rate: ${successCount}/${totalCount} (${successRate}%)`);

    // Group results by category
    const categories = {
      'OpenAI Format': allResults.filter(r => r.name.includes('OpenAI Format')),
      'Ollama Native': allResults.filter(r => r.name.includes('Ollama Native')),
      'ToolBridge Proxy': allResults.filter(r => r.name.includes('ToolBridge Proxy')),
    };

    Object.entries(categories).forEach(([category, results]) => {
      if (results.length === 0) {
        return;
      }

      const categorySuccess = results.filter(r => r.success).length;
      const categoryTotal = results.length;
      const categoryRate = Math.round((categorySuccess / categoryTotal) * 100);

      console.log(`\nðŸ”¸ ${category}: ${categorySuccess}/${categoryTotal} (${categoryRate}%)`);

      results.forEach(result => {
        const status = result.success ? 'âœ…' : 'âŒ';
        console.log(`  ${status} ${result.name}`);
        if (result.error) {
          console.log(`    Error: ${result.error}`);
        } else if (result.details) {
          console.log(`    Details: ${JSON.stringify(result.details, null, 2).slice(0, 200)}...`);
        }
      });
    });

    // Test specific type conversions
    console.log(`\n${separator}`);
    console.log('ðŸ”„ TYPE CONVERSION VERIFICATION');
    console.log(separator);

    const conversionTests = [
      'OpenAI â†’ Generic â†’ Ollama',
      'Ollama â†’ Generic â†’ OpenAI',
      'Streaming format conversions',
      'Tool call transformations',
      'Parameter mappings',
    ];

    console.log('\nâœ… Type conversions that should be working:');
    conversionTests.forEach(test => {
      console.log(`  âœ… ${test}`);
    });

    // Final assessment
    console.log(`\n${separator}`);
    console.log('ðŸŽ¯ FINAL ASSESSMENT');
    console.log(separator);

    if (successRate >= 80) {
      console.log(`\nðŸŽ‰ EXCELLENT: ${successRate}% success rate!`);
      console.log('âœ… Type conversion across formats is working well');
      console.log('âœ… All major API formats are properly supported');
      console.log('âœ… Streaming and non-streaming both work');
      console.log('âœ… Tool calling functionality is operational');
    } else if (successRate >= 60) {
      console.log(`\nâš ï¸  GOOD: ${successRate}% success rate`);
      console.log('âœ… Most type conversions are working');
      console.log('â“ Some issues may need attention');
    } else {
      console.log(`\nâŒ NEEDS WORK: ${successRate}% success rate`);
      console.log('âŒ Significant issues with type conversions');
      console.log('âŒ Format compatibility needs improvement');
    }

    console.log('\nðŸ“‹ Recommendations:');
    if (allResults.some(r => r.name.includes('Proxy') && !r.success)) {
      console.log('  - Start ToolBridge server to test proxy functionality');
    }
    if (allResults.some(r => r.name.includes('Streaming') && !r.success)) {
      console.log('  - Review streaming implementation');
    }
    if (allResults.some(r => r.name.includes('Tools') && !r.success)) {
      console.log('  - Check tool calling format conversions');
    }

  } catch (error) {
    console.error('âŒ Test setup failed:', error);
  } finally {
    // Cleanup servers
    console.log('\nðŸ§¹ Cleaning up servers...');
    servers.forEach(server => {
      try {
        (server as { close(): void }).close();
      } catch (_e) {
        // Ignore cleanup errors
      }
    });
    console.log('âœ… Cleanup complete');
  }
}

// Export for use as module
export { runComprehensiveFormatTests };

// Run if executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  runComprehensiveFormatTests().catch(console.error);
}
