# ---------- OpenAI Compatible ----------
# API Key for OpenAI or compatible provider (e.g., OpenRouter, LocalAI)
# If using Ollama locally, this key is often not required or ignored.
BACKEND_LLM_API_KEY=your_api_key_here

# ---------- Test Configuration ----------
# Set to true if you want to run integration tests against a real backend.
# WARNING: This may incur costs if using a paid API.
RUN_REAL_BACKEND_TESTS=false

# ---------- Ollama ----------
# URL for your local Ollama instance (default: http://localhost:11434)
# If you are running Ollama on a different host/port, update this in config.json
# or handle it via proxy settings.
# OLLAMA_URL=http://localhost:11434
